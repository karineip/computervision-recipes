{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Multi-Object Tracking Model\n",
    "\n",
    "In this notebook, we give an introduction to training an mult-object tracking model using [torchvision](https://pytorch.org/docs/stable/torchvision/index.html). Using a small dataset, we demonstrate how to train and evaluate a one-shot multi-object tracking model, which jointly detect objects and learn their re-ID features. In particular, we will use FairMOT, the one-shot tracking model developed by MSR Asia and others in this [repo](https://github.com/ifzhang/FairMOT).\n",
    "\n",
    "To learn more about how multi-object tracking works, visit our [FAQ](./FAQ.md).\n",
    "\n",
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the functions we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchVision: 0.4.0a0+6b959ee\n",
      "Torch is using GPU: Tesla K80\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Iterator\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from random import randrange\n",
    "from typing import Tuple\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import scrapbook as sb\n",
    "\n",
    "#KIP\n",
    "from utils_cv.multi_object_tracking.file_format import convert_vott_MOTxywh\n",
    "from utils_cv.multi_object_tracking.display_with_bb import convert_trackingbbox_video\n",
    "from ipywidgets import Video\n",
    "from utils_cv.tracking.dataset import TrackingDataset\n",
    "\n",
    "#from utils_cv.classification.data import Urls as UrlsIC\n",
    "#from utils_cv.common.data import unzip_url, data_path\n",
    "#from utils_cv.detection.data import Urls\n",
    "#from utils_cv.detection.dataset import DetectionDataset, get_transform\n",
    "# from utils_cv.detection.plot import (\n",
    "#     plot_grid,\n",
    "#     plot_boxes,\n",
    "#     plot_pr_curves,\n",
    "#     PlotSettings,\n",
    "#     plot_counts_curves,\n",
    "#     plot_detections\n",
    "# )\n",
    "# from utils_cv.detection.model import (\n",
    "#     DetectionLearner,\n",
    "#     get_pretrained_fasterrcnn,\n",
    "# )\n",
    "from utils_cv.common.gpu import which_processor, is_windows\n",
    "\n",
    "# Change matplotlib backend so that plots are shown for windows\n",
    "if is_windows():\n",
    "    plt.switch_backend(\"TkAgg\")\n",
    "\n",
    "print(f\"TorchVision: {torchvision.__version__}\")\n",
    "which_processor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows your machine's GPUs (if it has any) and the computing device `torch/torchvision` is using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure edits to libraries are loaded and plotting is shown in the notebook.\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, set some model runtime parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch device: cuda\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.0001\n",
    "#IM_SIZE = 500 #KIP: checked, for OD, DetectionLearner uses it if model is not None, else set to 500\n",
    "SAVE_MODEL = True\n",
    "\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using torch device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Prepare Datasets for training and object tracking evaluation\n",
    "\n",
    "## Annotated images for training\n",
    "\n",
    "In this section, for the training data, we use a toy dataset called *Fridge Objects*, which consists of 134 images of 4 classes of beverage container `{can, carton, milk bottle, water bottle}` photos taken on different backgrounds, as used for the object detection scenario. This will serve as a simple illustration of how finetuning a pre-trained tracking model with a small dataset can greatly enhance its performance.\n",
    "\n",
    "Similar to the object detection [training introduction notebook](../detection/01_training_introduction.ipynb), we use the helper function downloads and unzips data set to the `ComputerVision/data` directory. #TODO: correct if needed\n",
    "\n",
    "Set that directory in the `path` variable for ease of use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints', 'FridgeObjects.train', 'images', 'labels_with_ids']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH_TRAIN = \"./data/odFridgeObjects_FairMOTformat/\" #unzip_url(Urls.fridge_objects_path, exist_ok=True) #TODO\n",
    "path = Path(DATA_PATH_TRAIN)\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that we have two different folders inside, and a file:\n",
    "- `/images/`\n",
    "- `/labels_with_ids/\n",
    "- `/FridgeObjects.train`\n",
    "\n",
    "This format for object detection and object tracking is fairly common. Compared to object detection, for object tracking, the 'labels_with_ids' files have a field for the id number. \n",
    "\n",
    "```\n",
    "/data\n",
    "+-- images\n",
    "|   +-- 00001.jpg\n",
    "|   +-- 00002.jpg\n",
    "|   +-- ...\n",
    "+-- labels_with_ids\n",
    "|   +-- 00001.txt\n",
    "|   +-- 00002.txt\n",
    "|   +-- ...\n",
    "+-- ...\n",
    "```\n",
    "\n",
    "Each image corresponds to a txt file, which must have a similar name, e.g. txt file '00128.txt' contains detections and tracking  information in image file '00128.jpg', i.e. it contains the bounding boxes and the object ids information. In this example, our fridge object dataset is annotated in the format followed by the [FairMOT repo](https://github.com/ifzhang/FairMOT), originally from the [Towards-Realtime-MOT repo](https://github.com/Zhongdao/Towards-Realtime-MOT/blob/master/DATASET_ZOO.md). For example, '00128.txt' contains the following:\n",
    "\n",
    "```\n",
    "0 3 0.35671 0.50450 0.17635 0.23724\n",
    "0 2 0.67335 0.49399 0.36874 0.57057\n",
    "\n",
    "```\n",
    "This follows the FairMOT file format, where each line describes a bounding box as follows, as described in [Towards-Realtime-MOT repo](https://github.com/Zhongdao/Towards-Realtime-MOT/blob/master/DATASET_ZOO.md):\n",
    "```\n",
    "[class] [identity] [x_center] [y_center] [width] [height]\n",
    "```\n",
    "The field `class` is set to 0, for all, as only single-class multi-object tracking is supported.\n",
    "The field `identity` is an integer from `0` to `num_identities - 1`. In this training dataset, we used this dictionary to convert the original class-labels to ids: `{'milk_bottle': 0, 'water_bottle': 1, 'carton': 2, 'can': 3}`.\n",
    "The values of ` [x_center] [y_center] [width] [height]` are normalized by the width/height of the image, and range from `0` to `1`. \n",
    "\n",
    "In addition to the above images and labels files in their respective folder, FairMOT also requires an info file that lists the path (i.e. with the root path of the 'images' folder) of all image frames used for training. For instance, the first few lines of our info file, `FridgeObjects.train`, are:\n",
    "```\n",
    "./data/odFridgeObjects_FairMOTformat/images/00001.jpg\n",
    "./data/odFridgeObjects_FairMOTformat/images/00002.jpg\n",
    "./data/odFridgeObjects_FairMOTformat/images/00003.jpg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotated video segment for tracking evaluation (*new)\n",
    "In this section, we cover how to annotate the following video segment, in the scenario that we want to track the moving cans, which are similar to objects in the FridgeObject Dataset used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43df9705eb2f4b73ae109ab5253795a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Video(value=b'\\x00\\x00\\x00\\x18ftypmp42\\x00\\x00\\x00\\x00mp41isom\\x00\\x00\\x00(uuid\\\\\\xa7\\x08\\xfb2\\x8eB\\x05\\xa8ae\\â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "video_file = \"./data/carcans_1s.mp4\"\n",
    "\n",
    "video = Video.from_file(video_file)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use an annotation tool, such as VOTT, to annotate the cans. Please see the [FAQ](./FAQ.md) for more details on MOT annotation. For example, in the video, we can draw bounding boxes around the 2 cans, and tag them as `can_1` and `can_2`: \n",
    "<p align=\"center\">\n",
    "<img src=\"./media/carcans_vott_ui.png\" width=\"600\" align=\"center\"/>\n",
    "</p>\n",
    "\n",
    "Before annotating, make sure to set the extraction rate to 30fps, similar to that of the video. After annotation, you can export the annotation results in csv form, you will end up with the extracted frames as well as a csv file containing the bounding box and id info: ``` [image] [xmin] [y_min] [x_max] [y_max] [label]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Carcans_GT-export.csv',\n",
       " 'carcans_1s.mp4#t=0.033333.jpg',\n",
       " 'carcans_1s.mp4#t=0.066667.jpg',\n",
       " 'carcans_1s.mp4#t=0.1.jpg',\n",
       " 'carcans_1s.mp4#t=0.133333.jpg',\n",
       " 'carcans_1s.mp4#t=0.166667.jpg',\n",
       " 'carcans_1s.mp4#t=0.2.jpg',\n",
       " 'carcans_1s.mp4#t=0.233333.jpg',\n",
       " 'carcans_1s.mp4#t=0.266667.jpg',\n",
       " 'carcans_1s.mp4#t=0.3.jpg',\n",
       " 'carcans_1s.mp4#t=0.333333.jpg',\n",
       " 'carcans_1s.mp4#t=0.366667.jpg',\n",
       " 'carcans_1s.mp4#t=0.4.jpg',\n",
       " 'carcans_1s.mp4#t=0.433333.jpg',\n",
       " 'carcans_1s.mp4#t=0.466667.jpg',\n",
       " 'carcans_1s.mp4#t=0.5.jpg',\n",
       " 'carcans_1s.mp4#t=0.533333.jpg',\n",
       " 'carcans_1s.mp4#t=0.566667.jpg',\n",
       " 'carcans_1s.mp4#t=0.6.jpg',\n",
       " 'carcans_1s.mp4#t=0.633333.jpg',\n",
       " 'carcans_1s.mp4#t=0.666667.jpg',\n",
       " 'carcans_1s.mp4#t=0.7.jpg',\n",
       " 'carcans_1s.mp4#t=0.733333.jpg',\n",
       " 'carcans_1s.mp4#t=0.766667.jpg',\n",
       " 'carcans_1s.mp4#t=0.8.jpg',\n",
       " 'carcans_1s.mp4#t=0.833333.jpg',\n",
       " 'carcans_1s.mp4#t=0.866667.jpg',\n",
       " 'carcans_1s.mp4#t=0.9.jpg',\n",
       " 'carcans_1s.mp4#t=0.933333.jpg',\n",
       " 'carcans_1s.mp4#t=0.966667.jpg',\n",
       " 'carcans_1s.mp4#t=0.jpg',\n",
       " 'carcans_1s.mp4#t=1.jpg']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot_path = \"./data/carcans_vott-csv-export/\" #unzip_url(Urls.fridge_objects_path, exist_ok=True) #TODO\n",
    "path = Path(annot_path)\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the tracking performance will be carried out using the [py-motmetrics](https://github.com/cheind/py-motmetrics) repository, which provides multiple metrics for benchmarking multi-object trackers. Its `motmetrics` package takes in the ground-truth data in a format similar to that used in the [MOT challenge](https://motchallenge.net/), i.e.: \n",
    "```\n",
    "[frame number] [id number] [bbox left] [bbox top] [bbox width] [bbox height][Confidence score][Class][Visibility]\n",
    "```\n",
    "The last 3 columns can be set to -1 by default. To convert the VOTT annotation data to the MOT challenge format, you can use the following utility function,`convert_vott_MOTxywh()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH_EVAL_VOTT =  \"./data/carcans_vott_format/\"\n",
    "DATA_PATH_EVAL = \"./data/carcans_MOTformat/\"\n",
    "convert_vott_FairMOT(DATA_PATH_EVAL_VOTT, DATA_PATH_EVAL) #TODO KIP\n",
    "\n",
    "path = Path(DATA_PATH_EVAL)\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Dataset\n",
    "\n",
    "To load the data, we need to create a TorchvisionDataset object class that can be taken in by our `TrackingLearner` class wrapper, in a way that can recognized by the FairMOT repo code, saved in [multi_object_tracking/references folder](../../utils_cv/multi_object_tracking/references).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO check Casey's Dataset class format\n",
    "data_train = TrackingDataset(DATA_PATH_TRAIN, {\"custom\":\"./data/FridgeObjects.train\"}) #TODO: check with Casey why using a Dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune a Pretrained Model\n",
    "\n",
    "For the TrackingLearner, we use FairMOT's baseline tracking model, which used Torch's Adam algorithm as the default optimizer. \n",
    "\n",
    "FairMOT's baseline tracking model is pre-trained on pedestrian datasets, like the [MOT challenge datasets](https://motchallenge.net/). Hence, it does not even detect fridge objects, such as the in the evaluation video.\n",
    "\n",
    "When we initialize the TrackingLearner, we can pass in the training dataset. By default, the object will set the model to FairMOT's baseline tracking model. #TODO: add option for load_model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = TrackingLearner(data_train) \n",
    "print(f\"Model: {type(tracker.model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the training, we call the `fit(...)` method in the tracker object. The fit parameters include `num_epochs`, `lr`, `batch_size`.  If they are not set, they will by default be set to  `30`, `0.0001`, `12`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: hard-code the 10 epochs in code? coming from initialization of baseline checkpoint\n",
    "tracker.fit(num_epochs=EPOCHS+10, lr=LEARNING_RATE, bath_size=2) #KIP: Other params include lr_step, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate losses over the training epochs, and see how the model improves with training. Losses generated include detection-specific losses (e.g. `hm_loss`, `wh_loss`, `off_loss`) and id-specific losses (`id_loss`). The overall loss (`loss`) is a weighted average of the detection-specific and id-specific losses. We want to run the training for an appropriate `num_epochs` and `lr` (to be fine-tuned by the user) that produces a loss-curve that tails off. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.plot_training_losses() #TODO: add method to TrackingLearner class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict and evaluate tracking on a video\n",
    "Using the trained tracking model automatically stored in our `tracker` object, we can run the `predict(...)` method on our evaluation video dataset that we previously annotated, stored in path `DATA_PATH_EVAL`. There are several parameters that can be tweaked to improve the tracking: \n",
    "- conf_thres, det_thres, nms_thres, min_box_area: ...\n",
    "- track_buffer: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_w, input_h = get_image_size(DATA_PATH_EVAL, format=\"GT_FairMOT\") removed bug in FairMOT code? #TODO: Check with Casey regarding image resolution that's hardcoded\n",
    "conf_thres=0.6\n",
    "det_thres=0.3\n",
    "nms_thres=0.4\n",
    "track_buffer=frame_rate*10\n",
    "min_box_area=200\n",
    "track_results = tracker.predict(DATA_PATH_EVAL, conf_thres=conf_thres, det_thres=det_thres, nms_thres=nms_thres, track_buffer=track_buffer, min_box_area=min_box_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`track_results` is a dictionary, where each key is a the frame number, and the value is a list of `TrackingBbox` objects, which represent the tracking information of each object detected, i.e. bounding boxes and tracking ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(track_results)\n",
    "\n",
    "print(\"First frame...tracking result:\", track_results[0])\n",
    "print(\"Last frame...tracking result:\", track_results[-1])\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply pass on our `tracking_results` to the `evaluate()` method in our tracker to evaluate the results. Additionally, we pass on the path of the ground-truth data, on which we can run the evaluation. The result output is the MOT metrics, as developed in the [pymotemtric repo](https://github.com/cheind/py-motmetrics), which give a measure of different aspects of the tracking performance. Refer to the [FAQ](./FAQ.md) for more details. #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = tracker.evaluate(tracking_results, DATA_PATH_EVAL) #TODO: I defined evaluate API, check with Casey, about return form of predict (bboxes objects, vs csv)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the results, we can use the following utility function to produce a video with the bounding boxes and tracking ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_rate=30\n",
    "video_path = convert_trackingbbox_video(tracking_results, frame_rate) #TODO\n",
    "\n",
    "video = Video.from_file(video_path)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model\n",
    "The final step is to save the model to disk, where the save model is the baseline tracking model that has been finetuned on the FridgeObject dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_MODEL:\n",
    "    tracker.save(\"./models/finetuned_fridgeObjects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/scrapbook.scrap.json+json": {
       "data": false,
       "encoder": "json",
       "name": "skip_evaluation",
       "version": 1
      }
     },
     "metadata": {
      "scrapbook": {
       "data": true,
       "display": false,
       "name": "skip_evaluation"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/scrapbook.scrap.json+json": {
       "data": [
        0.35595571994781494,
        0.12577387690544128,
        0.05934049189090729,
        0.0445295050740242,
        0.03562961518764496,
        0.02832397259771824,
        0.027987590059638023,
        0.022436952218413353,
        0.024111101403832436,
        0.01963862217962742
       ],
       "encoder": "json",
       "name": "training_losses",
       "version": 1
      }
     },
     "metadata": {
      "scrapbook": {
       "data": true,
       "display": false,
       "name": "training_losses"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/scrapbook.scrap.json+json": {
       "data": [
        {
         "bbox": 0.43345073442423815
        },
        {
         "bbox": 0.758217688582397
        },
        {
         "bbox": 0.8295015488668323
        },
        {
         "bbox": 0.7984671657593753
        },
        {
         "bbox": 0.8308111190684818
        },
        {
         "bbox": 0.8277839969204616
        },
        {
         "bbox": 0.857568331133052
        },
        {
         "bbox": 0.8930010755505583
        },
        {
         "bbox": 0.901181367355771
        },
        {
         "bbox": 0.8938878642912632
        }
       ],
       "encoder": "json",
       "name": "training_average_precision",
       "version": 1
      }
     },
     "metadata": {
      "scrapbook": {
       "data": true,
       "display": false,
       "name": "training_average_precision"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preserve some of the notebook outputs\n",
    "sb.glue(\"training_losses\", tracker.losses)\n",
    "sb.glue(\"training_average_precision\", tracker.ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict tracking on a new video\n",
    "\n",
    "We can use the saved retrained tracking model to predict tracking in a new video, without outputting evaluation. Let us use the following video, which has not been annotated for ground-truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a2fe3711c4457cb574aecbc7d224b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Video(value=b'\\x00\\x00\\x00\\x18ftypmp42\\x00\\x00\\x00\\x00mp41isom\\x00\\x00\\x00(uuid\\\\\\xa7\\x08\\xfb2\\x8eB\\x05\\xa8ae\\â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_PATH_TEST = \"./data/carcans_8s.mp4\"\n",
    "\n",
    "video = Video.from_file(DATA_PATH_TEST)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to initialize a new `TrackingLearner` object and load the modelwe want, such as the one we saved above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = TrackingLearner(load_model=\"./models/finetuned_fridgeObjects\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_thres=0.6\n",
    "det_thres=0.3\n",
    "nms_thres=0.4\n",
    "track_buffer=frame_rate*10\n",
    "min_box_area=200\n",
    "track_results = tracker.predict(DATA_PATH_TEST, model=loaded_model, conf_thres=conf_thres, det_thres=det_thres, nms_thres=nms_thres, track_buffer=track_buffer, min_box_area=min_box_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_rate=30\n",
    "video_path = convert_trackingbbox_video(tracking_results, frame_rate) #TODO: ask about using ffmeg as cmd_str or other means, currently cv2\n",
    "\n",
    "video = Video.from_file(video_path)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the loaded model has been finetuned on cans, the tracking performance on this video which also contains cans, is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Using the concepts introduced in this notebook, you can bring your own dataset and train an object tracker to track objects of interest in a given video or image sequence. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "356.263px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
