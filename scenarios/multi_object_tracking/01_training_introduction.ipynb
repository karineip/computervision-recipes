{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Multi-Object Tracking Model\n",
    "\n",
    "In this notebook, we give an introduction to training a multi-object tracking model using [torchvision](https://pytorch.org/docs/stable/torchvision/index.html). Using a small dataset, we demonstrate how to train and evaluate a one-shot multi-object tracking model, which detects objects and learns their re-ID features. In particular, we will use FairMOT, the one-shot tracking model developed by MSR Asia and others in this [repo](https://github.com/ifzhang/FairMOT). We will train the model on a set of still images, then evaluate on a video. We also show how to save and load the trained model for inference on a second video. \n",
    "\n",
    "To learn more about how multi-object tracking works, visit our [FAQ](./FAQ.md).\n",
    "\n",
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the functions we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure edits to libraries are loaded and plotting is shown in the notebook.\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchVision: 0.4.0a0+6b959ee\n",
      "Torch is using GPU: Tesla K80\n"
     ]
    }
   ],
   "source": [
    "#Regular python libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import scrapbook as sb\n",
    "from ipywidgets import Video\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# Computer Vision repository\n",
    "sys.path.append(\"../../\")\n",
    "from utils_cv.multi_object_tracking.display_with_bb import convert_trackingbboxes_video\n",
    "from utils_cv.multi_object_tracking.file_format import convert_vott_MOTxywh \n",
    "from utils_cv.tracking.dataset import TrackingDataset \n",
    "from utils_cv.tracking.model import TrackingLearner \n",
    "from utils_cv.common.gpu import which_processor, is_windows\n",
    "\n",
    "# Change matplotlib backend so that plots are shown for windows\n",
    "if is_windows():\n",
    "    plt.switch_backend(\"TkAgg\")\n",
    "\n",
    "print(f\"TorchVision: {torchvision.__version__}\")\n",
    "which_processor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows your machine's GPUs (if it has any) and the computing device `torch/torchvision` is using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, set the data input and some model runtime parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Datasets \n",
    "DATA_PATH_TRAIN = \"./data/odFridgeObjects_FairMOTformat/\" #unzip_url(Urls.fridge_objects_path, exist_ok=True) #TODO\n",
    "DATA_PATH_EVAL_VIDEO = \"./data/carcans_1s.mp4\" #unzip_url(Urls.fridge_objects_path, exist_ok=True) #TODO\n",
    "DATA_PATH_EVAL_VOTT = \"./data/carcans_vott-csv-export/\" #unzip_url(Urls.fridge_objects_path, exist_ok=True) #TODO\n",
    "DATA_PATH_EVAL = \"./data/carcans_MOTformat/\" #unzip_url(Urls.fridge_objects_path, exist_ok=True) #TODO\n",
    "DATA_PATH_TEST = \"./data/carcans_8s.mp4\" #unzip_url(Urls.fridge_objects_path, exist_ok=True) #TODO\n",
    "FRAME_RATE = 30 # frame rate  for 1st inference video\n",
    "FRAME_RATE_2 = 30 # frame rate  for 2nd inference video\n",
    "\n",
    "# Model parameters\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.0001\n",
    "#IM_SIZE = 500 #KIP: checked, for OD, DetectionLearner uses it if model is not None, else set to 500\n",
    "SAVE_MODEL = True\n",
    "\n",
    "# Inference parameters\n",
    "CONF_THRES = 0.6 ; TRACK_BUFFER = FRAME_RATE*10\n",
    "INPUT_W = 1920; INPUT_H = 1080\n",
    "\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using torch device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Prepare Training Dataset\n",
    "\n",
    "In this section, for the training data, we use a toy dataset called *Fridge Objects*, which consists of 134 images of 4 classes of beverage container `{can, carton, milk bottle, water bottle}` photos taken on different backgrounds, as used for the object detection scenario. This will serve as a simple illustration of how finetuning a pre-trained tracking model with a small dataset to enhance its tracking performance.\n",
    "\n",
    "Similar to the object detection [training introduction notebook](../detection/01_training_introduction.ipynb), we use the helper function downloads and unzips data set to the `ComputerVision/data` directory. #TODO: correct if needed\n",
    "\n",
    "Set that directory in the `path` variable for ease of use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['images', '.ipynb_checkpoints', 'FridgeObjects.train', 'labels_with_ids']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path(DATA_PATH_TRAIN)\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that we have two different folders inside, and a file:\n",
    "- `/images/`\n",
    "- `/labels_with_ids/`\n",
    "- `/FridgeObjects.train`\n",
    "\n",
    "This format of having 2 folders, one for images and one for labels, is fairly common for object detection and object tracking. Compared to object detection, for object tracking, the 'labels_with_ids' files have a field for the id number. \n",
    "\n",
    "```\n",
    "/data\n",
    "+-- images\n",
    "|   +-- 00001.jpg\n",
    "|   +-- 00002.jpg\n",
    "|   +-- ...\n",
    "+-- labels_with_ids\n",
    "|   +-- 00001.txt\n",
    "|   +-- 00002.txt\n",
    "|   +-- ...\n",
    "+-- ...\n",
    "```\n",
    "\n",
    "Each image corresponds to a txt file, which must have a similar name, e.g. txt file '00128.txt' contains detections and tracking  information in image file '00128.jpg', i.e. it contains the bounding boxes and the object ids information. In this example, our fridge object dataset is annotated in the format followed by the [FairMOT repo](https://github.com/ifzhang/FairMOT), originally from the [Towards-Realtime-MOT repo](https://github.com/Zhongdao/Towards-Realtime-MOT/blob/master/DATASET_ZOO.md). For example, '00128.txt' contains the following:\n",
    "\n",
    "```\n",
    "0 3 0.35671 0.50450 0.17635 0.23724\n",
    "0 2 0.67335 0.49399 0.36874 0.57057\n",
    "\n",
    "```\n",
    "This follows the FairMOT file format, where each line describes a bounding box as follows, as described in [Towards-Realtime-MOT repo](https://github.com/Zhongdao/Towards-Realtime-MOT/blob/master/DATASET_ZOO.md):\n",
    "```\n",
    "[class] [identity] [x_center] [y_center] [width] [height]\n",
    "```\n",
    "The field `class` is set to 0, for all, as only single-class multi-object tracking is currently supported by the [FairMOT repo](https://github.com/ifzhang/FairMOT). The field `identity` is an integer from `0` to `num_identities - 1`. In this training dataset, we used this dictionary to convert the original class-labels to ids: `{'milk_bottle': 0, 'water_bottle': 1, 'carton': 2, 'can': 3}`. The values of ` [x_center] [y_center] [width] [height]` are normalized by the width/height of the image, and range from `0` to `1`. \n",
    "\n",
    "In addition to the above images and labels files in their respective folder, FairMOT also requires an info file that lists the path (i.e. with the root path of the 'images' folder) of all image frames used for training. For instance, the first few lines of our info file, `FridgeObjects.train`, are:\n",
    "```\n",
    "./data/odFridgeObjects_FairMOTformat/images/00001.jpg\n",
    "./data/odFridgeObjects_FairMOTformat/images/00002.jpg\n",
    "./data/odFridgeObjects_FairMOTformat/images/00003.jpg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Dataset\n",
    "\n",
    "To load the data, we need to create a Torchvision dataset object class using our `TrackingDataset` class wrapper, that also converts the dataset into the appropriate formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = TrackingDataset(DATA_PATH_TRAIN, {\"custom\":\"./data/FridgeObjects.train\"}) #TODO: Check with Casey, change input data format from FairMOT to xml, and use util funcs to convert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune a Pretrained Model\n",
    "\n",
    "For the TrackingLearner, we use FairMOT's baseline tracking model. FairMOT's baseline tracking model is pre-trained on pedestrian datasets, such as in the [MOT challenge datasets](https://motchallenge.net/). Hence, it does not even detect fridge objects, such as the in the evaluation video.\n",
    "\n",
    "When we initialize the TrackingLearner, we can pass in the training dataset. By default, the object will set the model to FairMOT's baseline tracking model. #TODO: add option for load_model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = TrackingLearner(data_train) \n",
    "print(f\"Model: {type(tracker.model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the training, we call the `fit(...)` method in the tracker object. The main fit parameters include `num_epochs`, `lr`, `batch_size`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: hard-code the 10 epochs in code? coming from initialization of baseline checkpoint\n",
    "tracker.fit(num_epochs=EPOCHS+10, lr=LEARNING_RATE, bath_size=2) #KIP: Other params include lr_step, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate losses over the training epochs, and see how the model improves with training. We want to run the training for an appropriate `num_epochs` and `lr` (to be fine-tuned by the user) that produces a loss-curve that tails off. The loss-curve for our training is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.plot_training_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Prepare Video Dataset for Tracking Evaluation\n",
    "In this section, we cover how to annotate a video segment for ground-truth to evaluate the trained model's tracking performance. For training, the dataset can consist of still images of unique objects that are properly labeled. These could be image sequences that do not necessarily have temporal meaning. On the other hand, for tracking prediction and evaluation, the dataset needs to be a video of image sequence with a temporal thread for the re-iding component of tracking to make sense. Please see the [Readme](./readme.md) for more information on the components of a tracking algorithm.  \n",
    "\n",
    "For the chosen video (below), we want to track the moving cans. These objects are similar to the objects in the FridgeObject Dataset used for training, hence the tracking algorithm will be trained to recognize them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = Video.from_file(DATA_PATH_EVAL_VIDEO)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use an annotation tool, such as VOTT, to annotate the cans. Please see the [FAQ](./FAQ.md) for more details on how to do the annotation. After annotation, the annotation files are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Carcans_GT-export.csv',\n",
       " 'carcans_1s.mp4#t=0.033333.jpg',\n",
       " 'carcans_1s.mp4#t=0.066667.jpg',\n",
       " 'carcans_1s.mp4#t=0.1.jpg',\n",
       " 'carcans_1s.mp4#t=0.133333.jpg',\n",
       " 'carcans_1s.mp4#t=0.166667.jpg',\n",
       " 'carcans_1s.mp4#t=0.2.jpg',\n",
       " 'carcans_1s.mp4#t=0.233333.jpg',\n",
       " 'carcans_1s.mp4#t=0.266667.jpg',\n",
       " 'carcans_1s.mp4#t=0.3.jpg',\n",
       " 'carcans_1s.mp4#t=0.333333.jpg',\n",
       " 'carcans_1s.mp4#t=0.366667.jpg',\n",
       " 'carcans_1s.mp4#t=0.4.jpg',\n",
       " 'carcans_1s.mp4#t=0.433333.jpg',\n",
       " 'carcans_1s.mp4#t=0.466667.jpg',\n",
       " 'carcans_1s.mp4#t=0.5.jpg',\n",
       " 'carcans_1s.mp4#t=0.533333.jpg',\n",
       " 'carcans_1s.mp4#t=0.566667.jpg',\n",
       " 'carcans_1s.mp4#t=0.6.jpg',\n",
       " 'carcans_1s.mp4#t=0.633333.jpg',\n",
       " 'carcans_1s.mp4#t=0.666667.jpg',\n",
       " 'carcans_1s.mp4#t=0.7.jpg',\n",
       " 'carcans_1s.mp4#t=0.733333.jpg',\n",
       " 'carcans_1s.mp4#t=0.766667.jpg',\n",
       " 'carcans_1s.mp4#t=0.8.jpg',\n",
       " 'carcans_1s.mp4#t=0.833333.jpg',\n",
       " 'carcans_1s.mp4#t=0.866667.jpg',\n",
       " 'carcans_1s.mp4#t=0.9.jpg',\n",
       " 'carcans_1s.mp4#t=0.933333.jpg',\n",
       " 'carcans_1s.mp4#t=0.966667.jpg',\n",
       " 'carcans_1s.mp4#t=0.jpg',\n",
       " 'carcans_1s.mp4#t=1.jpg']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path(DATA_PATH_EVAL_VOTT)\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the tracking performance will be carried out using the [py-motmetrics](https://github.com/cheind/py-motmetrics) repository, which provides multiple metrics for benchmarking multi-object trackers. Its `motmetrics` package takes in the ground-truth data in a format similar to that used in the MOT Challenge format (see [FAQ](./FAQ.md)). Thus, you can use the following utility function,`convert_vott_MOTxywh()`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_vott_MOTxywh(DATA_PATH_EVAL_VOTT, DATA_PATH_EVAL) #TODO: change if standardizing input to be xml files, KIP\n",
    "\n",
    "path = Path(DATA_PATH_EVAL)\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict and Evaluate Tracking\n",
    "\n",
    "To validate the trained model, we want to run it on the evaluation dataset we loaded above, and compare the predicted tracking results with the dataset's ground-truth annotations. \n",
    "\n",
    "Using the trained tracking model automatically stored in our `tracker` object, we can run the `predict` function on our evaluation video dataset that we previously annotated, stored in path `DATA_PATH_EVAL`. There are several parameters that can be tweaked to improve the tracking performance and inference speed, including `conf_thres`, `track_buffer`, `input_w`, `input_h`, please see the  [FAQ](./FAQ.md) for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_w, input_h, removed bug in FairMOT code? #TODO: Check with Casey regarding image resolution that's hardcoded\n",
    "track_results = tracker.predict(DATA_PATH_EVAL,\n",
    "                                conf_thres=CONF_THRES, track_buffer=TRACK_BUFFER,\n",
    "                                input_w= INPUT_W, input_h = INPUT_H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`track_results` is a dictionary, where each key is a the frame number, and the value is a list of `TrackingBbox` objects, which represent the tracking information of each object detected, i.e. bounding boxes and tracking ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(track_results)\n",
    "\n",
    "print(\"First frame...tracking result:\", track_results[0])\n",
    "print(\"Last frame...tracking result:\", track_results[-1])\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply pass on our `tracking_results` to the `evaluate()` method in our tracker to evaluate the results. Additionally, we pass on the path of the ground-truth data, on which we can run the evaluation. The result output is the MOT metrics, e.g. MOTA, IDF1 etc, as calculated by the [pymotmetric repo](https://github.com/cheind/py-motmetrics), which give a measure of different aspects of the tracking performance. Refer to the [FAQ](./FAQ.md) for more details. #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = tracker.evaluate(tracking_results, DATA_PATH_EVAL) #TODO: I defined evaluate API as shown in the cell below, check with Casey, about return form of predict (bboxes objects, vs csv)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate function for MOT scenario --> to put in TrackingLearner class\n",
    "from .references.fairmot.tracking_utils.evaluation import Evaluator\n",
    "from utils_cv.multi_object_tracking.display_with_bb import convert_trackingbboxes_txt\n",
    "def evaluate(self, prediction_dict: Dict, data_root_path: str ) -> pandas.DataFrame:\n",
    "    \"\"\" eval code that calls on 'motmetrics' package in referenced FairMOT script, to produce MOT metrics on inference, given ground-truth.\n",
    "        Args:\n",
    "            prediction_dict: dictionary of prediction results from predict() function, i.e. Dict[int, List[TrackingBbox]] \n",
    "            data_root_path: path of dataset containing GT annotations in MOTchallenge format (xywh)\n",
    "        Returns:\n",
    "            strsummary: pandas.DataFrame output by method in 'motmetrics', containing metrics scores\n",
    "        Raises:\n",
    "            Exception: if both `prediction_dict` and `self.stored_predictions` are None.\n",
    "        \"\"\"\n",
    "    if prediction_dict is None:\n",
    "        if not self.stored_predictions: #TODO: add stored_predictions in predict() maybe\n",
    "            raise Exception(\"No predict() function run on dataset for for evaluation\")\n",
    "       prediction_dict = self.stored_predictions\n",
    "\n",
    "    result_filename = os.path.join(data_root_path,'results', 'results.txt')\n",
    "    convert_trackingbboxes_txt(prediction_dict, result_filename) #TODO: KIP, or put in predict()\n",
    "\n",
    "    #Implementation inspired from code found here: https://github.com/ifzhang/FairMOT/blob/master/src/track.py\n",
    "    evaluator = Evaluator(gt_path, \"single_vid\", \"mot\")\n",
    "    accs=[evaluator.eval_file(result_filename)]    \n",
    "\n",
    "    # get summary\n",
    "    metrics = mm.metrics.motchallenge_metrics\n",
    "    mh = mm.metrics.create()\n",
    "    summary = Evaluator.get_summary(accs, (\"single_vid\"), metrics)\n",
    "    strsummary = mm.io.render_summary(\n",
    "        summary,\n",
    "        formatters=mh.formatters,\n",
    "        namemap=mm.io.motchallenge_metric_names\n",
    "    )\n",
    "    print(strsummary)\n",
    "    Evaluator.save_summary(summary, os.path.join(result_root,'results', 'summary_metrics.xlsx'))\n",
    "\n",
    "    return strsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To visualize the results, we can use the following utility function to produce a video with the bounding boxes and tracking ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_video_path = convert_trackingbboxes_video(tracking_results, video_to_track, frame_rate = FRAME_RATE)\n",
    "\n",
    "video = Video.from_file(tracking_video_path)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "The final step is to save the model to disk, with the model being the baseline tracking model that has been finetuned on the FridgeObject dataset. Use the `TrackingLearner` `save` function to save the model to the wanted path, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_MODEL:\n",
    "    tracker.save(\"./models/finetuned_fridgeObjects\") #TODO: check w Casey format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/scrapbook.scrap.json+json": {
       "data": false,
       "encoder": "json",
       "name": "skip_evaluation",
       "version": 1
      }
     },
     "metadata": {
      "scrapbook": {
       "data": true,
       "display": false,
       "name": "skip_evaluation"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/scrapbook.scrap.json+json": {
       "data": [
        0.35595571994781494,
        0.12577387690544128,
        0.05934049189090729,
        0.0445295050740242,
        0.03562961518764496,
        0.02832397259771824,
        0.027987590059638023,
        0.022436952218413353,
        0.024111101403832436,
        0.01963862217962742
       ],
       "encoder": "json",
       "name": "training_losses",
       "version": 1
      }
     },
     "metadata": {
      "scrapbook": {
       "data": true,
       "display": false,
       "name": "training_losses"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/scrapbook.scrap.json+json": {
       "data": [
        {
         "bbox": 0.43345073442423815
        },
        {
         "bbox": 0.758217688582397
        },
        {
         "bbox": 0.8295015488668323
        },
        {
         "bbox": 0.7984671657593753
        },
        {
         "bbox": 0.8308111190684818
        },
        {
         "bbox": 0.8277839969204616
        },
        {
         "bbox": 0.857568331133052
        },
        {
         "bbox": 0.8930010755505583
        },
        {
         "bbox": 0.901181367355771
        },
        {
         "bbox": 0.8938878642912632
        }
       ],
       "encoder": "json",
       "name": "training_average_precision",
       "version": 1
      }
     },
     "metadata": {
      "scrapbook": {
       "data": true,
       "display": false,
       "name": "training_average_precision"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preserve some of the notebook outputs\n",
    "sb.glue(\"training_losses\", tracker.losses)\n",
    "sb.glue(\"training_average_precision\", tracker.ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Tracking on a New Video\n",
    "\n",
    "We can use the saved retrained tracking model to predict tracking in a new video, without outputting evaluation. Let us use the following video, which has not been annotated for ground-truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f1abb921594afd889d7fd30ea324fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Video(value=b'\\x00\\x00\\x00\\x18ftypmp42\\x00\\x00\\x00\\x00mp41isom\\x00\\x00\\x00(uuid\\\\\\xa7\\x08\\xfb2\\x8eB\\x05\\xa8ae\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "video_to_track = Video.from_file(DATA_PATH_TEST)\n",
    "video_to_track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to initialize a new `TrackingLearner` object and load the modelwe want, such as the one we saved above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = TrackingLearner(load_model=\"./models/finetuned_fridgeObjects\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_results = tracker.predict(DATA_PATH_TEST,\n",
    "                                conf_thres=CONF_THRES, track_buffer=TRACK_BUFFER,\n",
    "                                input_w= INPUT_W, input_h = INPUT_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = convert_trackingbboxes_video(tracking_results, video_to_track, frame_rate = FRAME_RATE_2) \n",
    "video = Video.from_file(video_path)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the loaded model has been finetuned on cans, the tracking performance on this video, which also contains cans, is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Using the concepts introduced in this notebook, you can bring your own dataset and train an object tracker to track objects of interest in a given video or image sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "236.729px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
